# app/crawler/bfs_url_extractor.py

from bs4 import BeautifulSoup
import aiohttp
from urllib.parse import urljoin, urlparse
from app.repository.content_url_repository import ContentUrlRepository

MAX_DEPTH = 2  # íƒìƒ‰ ìµœëŒ€ ê¹Šì´
MAX_URLS_PER_SYMBOL = 10  # ì‹¬ë³¼ë‹¹ ìµœëŒ€ URL ìˆ˜ì§‘ ìˆ˜

async def fetch_html(session, url):
    try:
        async with session.get(url, timeout=5) as response:
            return await response.text()
    except Exception as e:
        print(f"âŒ ì‹¤íŒ¨: {url} - {e}")
        return None

async def extract_links(html, base_url):
    soup = BeautifulSoup(html, "lxml")
    links = set()
    for tag in soup.find_all("a", href=True):
        href = tag['href']
        absolute_url = urljoin(base_url, href)
        parsed = urlparse(absolute_url)
        if parsed.scheme.startswith("http"):
            links.add(absolute_url)
    return links

async def bfs_extract_urls(symbol: str) -> list[str]:
    print(f"[ğŸ” START] {symbol} BFS URL íƒìƒ‰ ì‹œì‘")

    visited = set()
    result = set()

    try:
        existing_urls = ContentUrlRepository.get_existing_urls_for_symbol(symbol)
        print(f"[ğŸ§¾ DB ì¡°íšŒ] {symbol} ê¸°ì¡´ URL ìˆ˜: {len(existing_urls)}")
    except Exception as e:
        print(f"[âŒ DB ì—ëŸ¬] {symbol} - {e}")
        return []

    queue = [f"https://news.google.com/search?q={symbol}"]
    async with aiohttp.ClientSession() as session:
        depth = 0
        while queue and depth < MAX_DEPTH and len(result) < MAX_URLS_PER_SYMBOL:
            print(f"[ğŸ” ê¹Šì´ {depth}] Queue í¬ê¸°: {len(queue)}")

            next_queue = []
            for url in queue:
                if url in visited:
                    continue
                visited.add(url)

                print(f"[ğŸŒ ìš”ì²­] {url}")
                html = await fetch_html(session, url)
                if not html:
                    continue

                links = await extract_links(html, url)
                print(f"[ğŸ”— ì¶”ì¶œëœ ë§í¬ ìˆ˜] {len(links)}")

                for link in links:
                    if "news" in link and link not in visited:
                        if link in existing_urls:
                            print(f"[âš ï¸ ì¤‘ë³µ URL ìŠ¤í‚µ] {link}")
                            continue

                        result.add(link)
                        next_queue.append(link)

                        print(f"[âœ… URL ìˆ˜ì§‘ë¨] {link}")
                        if len(result) >= MAX_URLS_PER_SYMBOL:
                            break
            queue = next_queue
            depth += 1

    print(f"[ğŸ ì™„ë£Œ] {symbol} â†’ ìµœì¢… URL ìˆ˜: {len(result)}")
    return list(result)