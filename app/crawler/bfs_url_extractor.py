from bs4 import BeautifulSoup
import requests
from urllib.parse import urljoin, urlparse
import xml.etree.ElementTree as ET
from app.repository.content_url_repository import ContentUrlRepository

MAX_DEPTH = 2
MAX_URLS_PER_SYMBOL = 3

def fetch_html(url):
    try:
        response = requests.get(url, timeout=5, headers={"User-Agent": "Mozilla/5.0"})
        if response.status_code == 200:
            return response.text
        else:
            print(f"âŒ ì‹¤íŒ¨: {url} - ìƒíƒœì½”ë“œ {response.status_code}")
            return None
    except Exception as e:
        print(f"âŒ ì˜ˆì™¸: {url} - {e}")
        return None

def extract_links(html, base_url):
    soup = BeautifulSoup(html, "lxml")
    links = set()
    for tag in soup.find_all("a", href=True):
        href = tag["href"]
        absolute_url = urljoin(base_url, href)
        parsed = urlparse(absolute_url)
        if parsed.scheme.startswith("http"):
            links.add(absolute_url)
    return links

def extract_title_summary(symbol, html, url):
    soup = BeautifulSoup(html, "lxml")
    title = soup.title.string.strip() if soup.title and soup.title.string else "ì œëª© ì—†ìŒ"
    meta = soup.find("meta", attrs={"name": "description"}) or soup.find("meta", attrs={"property": "og:description"})
    summary = meta['content'].strip() if meta and meta.get('content') else "ìš”ì•½ ì—†ìŒ"
    return {
        "symbol": symbol,
        "title": title,
        "summary": summary,
        "url": url
    }

def get_initial_links_from_rss(symbol: str) -> list[str]:
    """
    RSSì—ì„œ ë‰´ìŠ¤ URLë§Œ ì¶”ì¶œí•˜ì—¬ ì´ˆê¸° íë¡œ ì‚¬ìš©
    """
    rss_url = f"https://news.google.com/rss/search?q={symbol}"
    headers = {
        "User-Agent": "Mozilla/5.0"
    }

    try:
        response = requests.get(rss_url, headers=headers)
        response.raise_for_status()
        root = ET.fromstring(response.content)
        channel = root.find("channel")
        items = channel.findall("item")
        return [item.find("link").text for item in items]
    except Exception as e:
        print(f"[âŒ RSS íŒŒì‹± ì‹¤íŒ¨] {e}")
        return []

def bfs_extract_urls(symbol: str) -> list[dict]:
    print(f"[ğŸ” START] {symbol} BFS URL íƒìƒ‰ ì‹œì‘")

    visited = set()
    results = []

    try:
        existing_urls = ContentUrlRepository.get_existing_urls_for_symbol(symbol)
        print(f"[ğŸ§¾ DB ì¡°íšŒ] {symbol} ê¸°ì¡´ URL ìˆ˜: {len(existing_urls)}")
    except Exception as e:
        print(f"[âŒ DB ì—ëŸ¬] {symbol} - {e}")
        existing_urls = set()

    # âœ… ì´ˆê¸° íëŠ” RSSì—ì„œ ì¶”ì¶œëœ URLë“¤
    rss_links = get_initial_links_from_rss(symbol)
    queue = [(url, 0) for url in rss_links]

    while queue and len(results) < MAX_URLS_PER_SYMBOL:
        url, depth = queue.pop(0)
        if url in visited or depth > MAX_DEPTH:
            continue
        visited.add(url)

        print(f"[ğŸŒ ìš”ì²­] ê¹Šì´ {depth} â†’ {url}")
        html = fetch_html(url)
        if not html:
            continue

        try:
            data = extract_title_summary(symbol, html, url)
            if url not in existing_urls:
                results.append(data)
                print(f"[âœ… ìˆ˜ì§‘ë¨] {data['title']} - {url}")
                if len(results) >= MAX_URLS_PER_SYMBOL:
                    break
        except Exception as e:
            print(f"[âš ï¸ ìš”ì•½ ì¶”ì¶œ ì‹¤íŒ¨] {url} - {e}")

        # ë§í¬ ì¶”ì¶œ (BFS íƒìƒ‰ í™•ì¥)
        links = extract_links(html, url)
        print(f"[ğŸ”— ì¶”ì¶œëœ ë§í¬ ìˆ˜] {len(links)}")
        for link in links:
            if "news" in link and link not in visited and link not in existing_urls:
                queue.append((link, depth + 1))

    print(f"[ğŸ ì™„ë£Œ] {symbol} â†’ ìµœì¢… URL ìˆ˜: {len(results)}")
    return results


if __name__ == "__main__":
    import sys
    import json

    if len(sys.argv) < 2:
        print("âŒ ì‚¬ìš©ë²•: python -m app.crawler.bfs_url_extractor <ì‹¬ë³¼>")
        sys.exit(1)

    symbol = sys.argv[1]
    results = bfs_extract_urls(symbol)

    print("\nğŸ“¦ ìµœì¢… ìˆ˜ì§‘ ê²°ê³¼:")
    for i, item in enumerate(results, 1):
        print(f"[{i}] {item['title']}")
        print(f"    ğŸ”— URL: {item['url']}")
        print(f"    ğŸ“ ìš”ì•½: {item['summary']}\n")

    # JSON ì „ì²´ ë³´ê¸° ì›í•˜ë©´ ì•„ë˜ë„ ê°€ëŠ¥
    # print(json.dumps(results, indent=2, ensure_ascii=False))
